{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Iris Species\n",
    "In this section, we will go through a simple machine learning application and create\n",
    "our first model. In the process, we will introduce some core concepts and terms.\n",
    "\n",
    "Let’s assume that a hobby botanist is interested in distinguishing the species of some\n",
    "iris flowers that she has found. She has collected some measurements associated with\n",
    "each iris: the length and width of the petals and the length and width of the sepals, all\n",
    "measured in centimeters.\n",
    "\n",
    "She also has the measurements of some irises that have been previously identified by\n",
    "an expert botanist as belonging to the species setosa, versicolor, or virginica. For these\n",
    "measurements, she can be certain of which species each iris belongs to. Let’s assume\n",
    "that these are the only species our hobby botanist will encounter in the wild.\n",
    "\n",
    "Our goal is to build a machine learning model that can learn from the measurements\n",
    "of these irises whose species is known, so that we can predict the species for a new\n",
    "iris.\n",
    "\n",
    "Reference: Introduction to Machine learning with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "iris = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to build a machine learning model from this data that can predict the species\n",
    "of iris for a new set of measurements. But before we can apply our model to new\n",
    "measurements, we need to know whether it actually works—that is, whether we\n",
    "should trust its predictions.\n",
    "\n",
    "To assess the model’s performance, we show it new data (data that it hasn’t seen\n",
    "before) for which we have labels. This is usually done by splitting the labeled data we\n",
    "have collected (here, our 150 flower measurements) into two parts. One part of the\n",
    "data is used to build our machine learning model, and is called the training data or\n",
    "training set. The rest of the data will be used to assess how well the model works; this\n",
    "is called the test data, test set, or hold-out set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn contains a function that shuffles the dataset and splits it for you: the\n",
    "train_test_split function. This function extracts 75% of the rows in the data as the\n",
    "training set, together with the corresponding labels for this data. The remaining 25%\n",
    "of the data, together with the remaining labels, is declared as the test set. Deciding\n",
    "how much data you want to put into the training and the test set respectively is somewhat\n",
    "arbitrary, but using a test set containing 25% of the data is a good rule of thumb.\n",
    "\n",
    "In scikit-learn, data is usually denoted with a capital X, while labels are denoted by\n",
    "a lowercase y. This is inspired by the standard formulation f(x)=y in mathematics,\n",
    "where x is the input to a function and y is the output. Following more conventions\n",
    "from mathematics, we use a capital X because the data is a two-dimensional array (a\n",
    "matrix) and a lowercase y because the target is a one-dimensional array (a vector).\n",
    "Let’s call train_test_split on our data and assign the outputs using this nomenclature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "[code ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before making the split, the train_test_split function shuffles the dataset using a\n",
    "pseudorandom number generator. If we just took the last 25% of the data as a test set,\n",
    "all the data points would have the label 2, as the data points are sorted by the label\n",
    "(see the output for iris['target'] shown earlier). Using a test set containing only\n",
    "one of the three classes would not tell us much about how well our model generalizes,\n",
    "so we shuffle our data to make sure the test data contains data from all classes.\n",
    "\n",
    "To make sure that we will get the same output if we run the same function several\n",
    "times, we provide the pseudorandom number generator with a fixed seed using the\n",
    "random_state parameter. This will make the outcome deterministic, so this line will\n",
    "always have the same outcome. We will always fix the random_state in this way when\n",
    "using randomized procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the train_test_split function is X_train, X_test, y_train, and\n",
    "y_test, which are all NumPy arrays. X_train contains 75% of the rows of the dataset,\n",
    "and X_test contains the remaining 25%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (112, 4)\n",
      "y_train shape: (112,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (38, 4)\n",
      "y_test shape: (38,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start building the actual machine learning model. There are many classification\n",
    "algorithms in scikit-learn that we could use. Here we will use a k-nearest\n",
    "neighbors classifier, which is easy to understand. Building this model only consists of\n",
    "storing the training set. To make a prediction for a new data point, the algorithm\n",
    "finds the point in the training set that is closest to the new point. Then it assigns the\n",
    "label of this training point to the new data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All machine learning models in scikit-learn are implemented in their own classes,\n",
    "which are called Estimator classes. The k-nearest neighbors classification algorithm\n",
    "is implemented in the KNeighborsClassifier class in the neighbors module. Before\n",
    "we can use the model, we need to instantiate the class into an object. This is when we\n",
    "will set any parameters of the model. The most important parameter of KNeighbor\n",
    "sClassifier is the number of neighbors, which we will set to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "[code ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the model on the training set, we call the fit method of the knn object,\n",
    "which takes as arguments the NumPy array X_train containing the training data and\n",
    "the NumPy array y_train of the corresponding training labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[code ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make predictions using this model on new data for which we might not\n",
    "know the correct labels. Imagine we found an iris in the wild with a sepal length of\n",
    "5 cm, a sepal width of 2.9 cm, a petal length of 1 cm, and a petal width of 0.2 cm.\n",
    "What species of iris would this be? We can put this data into a NumPy array, again by\n",
    "calculating the shape—that is, the number of samples (1) multiplied by the number of\n",
    "features (4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_new.shape: (1, 4)\n"
     ]
    }
   ],
   "source": [
    "X_new = np.array([[5, 2.9, 1, 0.2]])\n",
    "print(\"X_new.shape: {}\".format(X_new.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict the result label of X_new:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[code ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model predicts that this new iris belongs to the class 0, meaning its species is\n",
    "setosa. But how do we know whether we can trust our model? We don’t know the correct\n",
    "species of this sample, which is the whole point of building the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the test set that we created earlier comes in. This data was not used to\n",
    "build the model, but we do know what the correct species is for each iris in the test\n",
    "set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[code ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning with Cross Validation (CV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we’ll explore a CV method that can be used to tune the hyperparameter K using the above training and test data.\n",
    "\n",
    "Scikit-learn comes in handy with its cross_val_score() method. We specifiy that we are performing 10 folds with the cv=10 parameter and that our scoring metric should be accuracy since we are in a classification setting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "neighbors = list(range(1,10))\n",
    "cv_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[code ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the misclassification error versus K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[code ...]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
